{"count":10,"models":[{"bigcodebench_full":78.2,"bigcodebench_hard":65.8,"composite_score":74.38,"context_length":200000,"description":"Claude Opus 4.5 is Anthropic\u2019s frontier reasoning model optimized for complex software engineering, agentic workflows, and long-horizon computer use. It offers strong multimodal capabilities, competitive performance across real-world coding and reasoning benchmarks, and improved robustness to prompt injection. The model is designed to operate efficiently across varied effort levels, enabling developers to trade off speed, depth, and token usage depending on task requirements. It comes with a new parameter to control token efficiency, which can be accessed using the OpenRouter Verbosity parameter with low, medium, or high.\n\nOpus 4.5 supports advanced tool use, extended context management, and coordinated multi-agent setups, making it well-suited for autonomous research, debugging, multi-step planning, and spreadsheet/browser manipulation. It delivers substantial gains in structured reasoning, execution reliability, and alignment compared to prior Opus generations, while reducing token overhead and improving performance on long-running tasks.","huggingface_id":"","humaneval_plus":95.8,"is_free":false,"livebench_coding":68.5,"livecodebench":56.8,"mbpp_plus":82.5,"model_id":"anthropic/claude-opus-4.5","model_name":"Anthropic: Claude Opus 4.5","openrouter_id":"anthropic/claude-opus-4.5","price_per_million_input":5.0,"price_per_million_output":25.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":52.8,"swe_bench_verified":58.5},{"bigcodebench_full":78.2,"bigcodebench_hard":65.8,"composite_score":74.38,"context_length":200000,"description":"Claude Opus 4.1 is an updated version of Anthropic\u2019s flagship model, offering improved performance in coding, reasoning, and agentic tasks. It achieves 74.5% on SWE-bench Verified and shows notable gains in multi-file code refactoring, debugging precision, and detail-oriented reasoning. The model supports extended thinking up to 64K tokens and is optimized for tasks involving research, data analysis, and tool-assisted reasoning.","huggingface_id":"","humaneval_plus":95.8,"is_free":false,"livebench_coding":68.5,"livecodebench":56.8,"mbpp_plus":82.5,"model_id":"anthropic/claude-opus-4.1","model_name":"Anthropic: Claude Opus 4.1","openrouter_id":"anthropic/claude-opus-4.1","price_per_million_input":15.0,"price_per_million_output":75.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":52.8,"swe_bench_verified":58.5},{"bigcodebench_full":78.2,"bigcodebench_hard":65.8,"composite_score":74.38,"context_length":200000,"description":"Claude Opus 4 is benchmarked as the world\u2019s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. \n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)","huggingface_id":"","humaneval_plus":95.8,"is_free":false,"livebench_coding":68.5,"livecodebench":56.8,"mbpp_plus":82.5,"model_id":"anthropic/claude-opus-4","model_name":"Anthropic: Claude Opus 4","openrouter_id":"anthropic/claude-opus-4","price_per_million_input":15.0,"price_per_million_output":75.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":52.8,"swe_bench_verified":58.5},{"bigcodebench_full":74.8,"bigcodebench_hard":62.5,"composite_score":71.88,"context_length":1000000,"description":"Claude Sonnet 4.5 is Anthropic\u2019s most advanced Sonnet model to date, optimized for real-world agents and coding workflows. It delivers state-of-the-art performance on coding benchmarks such as SWE-bench Verified, with improvements across system design, code security, and specification adherence. The model is designed for extended autonomous operation, maintaining task continuity across sessions and providing fact-based progress tracking.\n\nSonnet 4.5 also introduces stronger agentic capabilities, including improved tool orchestration, speculative parallel execution, and more efficient context and memory management. With enhanced context tracking and awareness of token usage across tool calls, it is particularly well-suited for multi-context and long-running workflows. Use cases span software engineering, cybersecurity, financial analysis, research agents, and other domains requiring sustained reasoning and tool use.","huggingface_id":"","humaneval_plus":94.5,"is_free":false,"livebench_coding":65.2,"livecodebench":52.5,"mbpp_plus":81.2,"model_id":"anthropic/claude-sonnet-4.5","model_name":"Anthropic: Claude Sonnet 4.5","openrouter_id":"anthropic/claude-sonnet-4.5","price_per_million_input":3.0,"price_per_million_output":15.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":49.8,"swe_bench_verified":55.2},{"bigcodebench_full":74.8,"bigcodebench_hard":62.5,"composite_score":71.88,"context_length":1000000,"description":"Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)","huggingface_id":"","humaneval_plus":94.5,"is_free":false,"livebench_coding":65.2,"livecodebench":52.5,"mbpp_plus":81.2,"model_id":"anthropic/claude-sonnet-4","model_name":"Anthropic: Claude Sonnet 4","openrouter_id":"anthropic/claude-sonnet-4","price_per_million_input":3.0,"price_per_million_output":15.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":49.8,"swe_bench_verified":55.2},{"bigcodebench_full":74.2,"bigcodebench_hard":62.5,"composite_score":70.97,"context_length":200000,"description":"OpenAI o3-mini-high is the same model as [o3-mini](/openai/o3-mini) with reasoning_effort set to high. \n\no3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding. The model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.\n\nThe model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.","huggingface_id":"","humaneval_plus":94.2,"is_free":false,"livebench_coding":68.5,"livecodebench":58.5,"mbpp_plus":83.5,"model_id":"openai/o3-mini-high","model_name":"OpenAI: o3 Mini High","openrouter_id":"openai/o3-mini-high","price_per_million_input":1.1,"price_per_million_output":4.4,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":43.2,"swe_bench_verified":48.5},{"bigcodebench_full":74.2,"bigcodebench_hard":62.5,"composite_score":70.97,"context_length":200000,"description":"OpenAI o3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding.\n\nThis model supports the `reasoning_effort` parameter, which can be set to \"high\", \"medium\", or \"low\" to control the thinking time of the model. The default is \"medium\". OpenRouter also offers the model slug `openai/o3-mini-high` to default the parameter to \"high\".\n\nThe model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.\n\nThe model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.","huggingface_id":"","humaneval_plus":94.2,"is_free":false,"livebench_coding":68.5,"livecodebench":58.5,"mbpp_plus":83.5,"model_id":"openai/o3-mini","model_name":"OpenAI: o3 Mini","openrouter_id":"openai/o3-mini","price_per_million_input":1.1,"price_per_million_output":4.4,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":43.2,"swe_bench_verified":48.5},{"bigcodebench_full":72.5,"bigcodebench_hard":59.8,"composite_score":69.06,"context_length":1048576,"description":"Gemini 2.5 Pro is Google\u2019s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs \u201cthinking\u201d capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.","huggingface_id":"","humaneval_plus":93.2,"is_free":false,"livebench_coding":64.5,"livecodebench":52.8,"mbpp_plus":80.5,"model_id":"google/gemini-2.5-pro","model_name":"Google: Gemini 2.5 Pro","openrouter_id":"google/gemini-2.5-pro","price_per_million_input":1.25,"price_per_million_output":10.0,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":43.5,"swe_bench_verified":48.2},{"bigcodebench_full":72.5,"bigcodebench_hard":59.8,"composite_score":69.06,"context_length":1048576,"description":"Gemini 2.5 Pro is Google\u2019s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs \u201cthinking\u201d capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.\n","huggingface_id":"","humaneval_plus":93.2,"is_free":false,"livebench_coding":64.5,"livecodebench":52.8,"mbpp_plus":80.5,"model_id":"google/gemini-2.5-pro-preview","model_name":"Google: Gemini 2.5 Pro Preview 06-05","openrouter_id":"google/gemini-2.5-pro-preview","price_per_million_input":1.25,"price_per_million_output":10.0,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":43.5,"swe_bench_verified":48.2},{"bigcodebench_full":72.5,"bigcodebench_hard":59.8,"composite_score":69.06,"context_length":1048576,"description":"Gemini 2.5 Pro is Google\u2019s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs \u201cthinking\u201d capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.","huggingface_id":"","humaneval_plus":93.2,"is_free":false,"livebench_coding":64.5,"livecodebench":52.8,"mbpp_plus":80.5,"model_id":"google/gemini-2.5-pro-preview-05-06","model_name":"Google: Gemini 2.5 Pro Preview 05-06","openrouter_id":"google/gemini-2.5-pro-preview-05-06","price_per_million_input":1.25,"price_per_million_output":10.0,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":43.5,"swe_bench_verified":48.2}],"success":true,"weights_used":null}
