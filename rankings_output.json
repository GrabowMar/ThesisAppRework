{"count":95,"rankings":[{"bigcodebench_full":78.2,"bigcodebench_hard":65.8,"composite_score":74.38,"context_length":200000,"description":"Claude Opus 4.5 is Anthropic\u2019s frontier reasoning model optimized for complex software engineering, agentic workflows, and long-horizon computer use. It offers strong multimodal capabilities, competitive performance across real-world coding and reasoning benchmarks, and improved robustness to prompt injection. The model is designed to operate efficiently across varied effort levels, enabling developers to trade off speed, depth, and token usage depending on task requirements. It comes with a new parameter to control token efficiency, which can be accessed using the OpenRouter Verbosity parameter with low, medium, or high.\n\nOpus 4.5 supports advanced tool use, extended context management, and coordinated multi-agent setups, making it well-suited for autonomous research, debugging, multi-step planning, and spreadsheet/browser manipulation. It delivers substantial gains in structured reasoning, execution reliability, and alignment compared to prior Opus generations, while reducing token overhead and improving performance on long-running tasks.","huggingface_id":"","humaneval_plus":95.8,"is_free":false,"livebench_coding":68.5,"livecodebench":56.8,"mbpp_plus":82.5,"model_id":"anthropic/claude-opus-4.5","model_name":"Anthropic: Claude Opus 4.5","openrouter_id":"anthropic/claude-opus-4.5","price_per_million_input":5.0,"price_per_million_output":25.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":52.8,"swe_bench_verified":58.5},{"bigcodebench_full":78.2,"bigcodebench_hard":65.8,"composite_score":74.38,"context_length":200000,"description":"Claude Opus 4.1 is an updated version of Anthropic\u2019s flagship model, offering improved performance in coding, reasoning, and agentic tasks. It achieves 74.5% on SWE-bench Verified and shows notable gains in multi-file code refactoring, debugging precision, and detail-oriented reasoning. The model supports extended thinking up to 64K tokens and is optimized for tasks involving research, data analysis, and tool-assisted reasoning.","huggingface_id":"","humaneval_plus":95.8,"is_free":false,"livebench_coding":68.5,"livecodebench":56.8,"mbpp_plus":82.5,"model_id":"anthropic/claude-opus-4.1","model_name":"Anthropic: Claude Opus 4.1","openrouter_id":"anthropic/claude-opus-4.1","price_per_million_input":15.0,"price_per_million_output":75.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":52.8,"swe_bench_verified":58.5},{"bigcodebench_full":78.2,"bigcodebench_hard":65.8,"composite_score":74.38,"context_length":200000,"description":"Claude Opus 4 is benchmarked as the world\u2019s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. \n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)","huggingface_id":"","humaneval_plus":95.8,"is_free":false,"livebench_coding":68.5,"livecodebench":56.8,"mbpp_plus":82.5,"model_id":"anthropic/claude-opus-4","model_name":"Anthropic: Claude Opus 4","openrouter_id":"anthropic/claude-opus-4","price_per_million_input":15.0,"price_per_million_output":75.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":52.8,"swe_bench_verified":58.5},{"bigcodebench_full":74.8,"bigcodebench_hard":62.5,"composite_score":71.88,"context_length":1000000,"description":"Claude Sonnet 4.5 is Anthropic\u2019s most advanced Sonnet model to date, optimized for real-world agents and coding workflows. It delivers state-of-the-art performance on coding benchmarks such as SWE-bench Verified, with improvements across system design, code security, and specification adherence. The model is designed for extended autonomous operation, maintaining task continuity across sessions and providing fact-based progress tracking.\n\nSonnet 4.5 also introduces stronger agentic capabilities, including improved tool orchestration, speculative parallel execution, and more efficient context and memory management. With enhanced context tracking and awareness of token usage across tool calls, it is particularly well-suited for multi-context and long-running workflows. Use cases span software engineering, cybersecurity, financial analysis, research agents, and other domains requiring sustained reasoning and tool use.","huggingface_id":"","humaneval_plus":94.5,"is_free":false,"livebench_coding":65.2,"livecodebench":52.5,"mbpp_plus":81.2,"model_id":"anthropic/claude-sonnet-4.5","model_name":"Anthropic: Claude Sonnet 4.5","openrouter_id":"anthropic/claude-sonnet-4.5","price_per_million_input":3.0,"price_per_million_output":15.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":49.8,"swe_bench_verified":55.2},{"bigcodebench_full":74.8,"bigcodebench_hard":62.5,"composite_score":71.88,"context_length":1000000,"description":"Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)","huggingface_id":"","humaneval_plus":94.5,"is_free":false,"livebench_coding":65.2,"livecodebench":52.5,"mbpp_plus":81.2,"model_id":"anthropic/claude-sonnet-4","model_name":"Anthropic: Claude Sonnet 4","openrouter_id":"anthropic/claude-sonnet-4","price_per_million_input":3.0,"price_per_million_output":15.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":49.8,"swe_bench_verified":55.2},{"bigcodebench_full":74.2,"bigcodebench_hard":62.5,"composite_score":70.97,"context_length":200000,"description":"OpenAI o3-mini-high is the same model as [o3-mini](/openai/o3-mini) with reasoning_effort set to high. \n\no3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding. The model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.\n\nThe model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.","huggingface_id":"","humaneval_plus":94.2,"is_free":false,"livebench_coding":68.5,"livecodebench":58.5,"mbpp_plus":83.5,"model_id":"openai/o3-mini-high","model_name":"OpenAI: o3 Mini High","openrouter_id":"openai/o3-mini-high","price_per_million_input":1.1,"price_per_million_output":4.4,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":43.2,"swe_bench_verified":48.5},{"bigcodebench_full":74.2,"bigcodebench_hard":62.5,"composite_score":70.97,"context_length":200000,"description":"OpenAI o3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding.\n\nThis model supports the `reasoning_effort` parameter, which can be set to \"high\", \"medium\", or \"low\" to control the thinking time of the model. The default is \"medium\". OpenRouter also offers the model slug `openai/o3-mini-high` to default the parameter to \"high\".\n\nThe model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.\n\nThe model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.","huggingface_id":"","humaneval_plus":94.2,"is_free":false,"livebench_coding":68.5,"livecodebench":58.5,"mbpp_plus":83.5,"model_id":"openai/o3-mini","model_name":"OpenAI: o3 Mini","openrouter_id":"openai/o3-mini","price_per_million_input":1.1,"price_per_million_output":4.4,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":43.2,"swe_bench_verified":48.5},{"bigcodebench_full":72.5,"bigcodebench_hard":59.8,"composite_score":69.06,"context_length":1048576,"description":"Gemini 2.5 Pro is Google\u2019s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs \u201cthinking\u201d capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.","huggingface_id":"","humaneval_plus":93.2,"is_free":false,"livebench_coding":64.5,"livecodebench":52.8,"mbpp_plus":80.5,"model_id":"google/gemini-2.5-pro","model_name":"Google: Gemini 2.5 Pro","openrouter_id":"google/gemini-2.5-pro","price_per_million_input":1.25,"price_per_million_output":10.0,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":43.5,"swe_bench_verified":48.2},{"bigcodebench_full":72.5,"bigcodebench_hard":59.8,"composite_score":69.06,"context_length":1048576,"description":"Gemini 2.5 Pro is Google\u2019s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs \u201cthinking\u201d capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.\n","huggingface_id":"","humaneval_plus":93.2,"is_free":false,"livebench_coding":64.5,"livecodebench":52.8,"mbpp_plus":80.5,"model_id":"google/gemini-2.5-pro-preview","model_name":"Google: Gemini 2.5 Pro Preview 06-05","openrouter_id":"google/gemini-2.5-pro-preview","price_per_million_input":1.25,"price_per_million_output":10.0,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":43.5,"swe_bench_verified":48.2},{"bigcodebench_full":72.5,"bigcodebench_hard":59.8,"composite_score":69.06,"context_length":1048576,"description":"Gemini 2.5 Pro is Google\u2019s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs \u201cthinking\u201d capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.","huggingface_id":"","humaneval_plus":93.2,"is_free":false,"livebench_coding":64.5,"livecodebench":52.8,"mbpp_plus":80.5,"model_id":"google/gemini-2.5-pro-preview-05-06","model_name":"Google: Gemini 2.5 Pro Preview 05-06","openrouter_id":"google/gemini-2.5-pro-preview-05-06","price_per_million_input":1.25,"price_per_million_output":10.0,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":43.5,"swe_bench_verified":48.2},{"bigcodebench_full":70.5,"bigcodebench_hard":58.8,"composite_score":68.87,"context_length":163840,"description":"DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI\u2019s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2\u00d7 faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.","huggingface_id":"tngtech/DeepSeek-TNG-R1T2-Chimera","humaneval_plus":93.5,"is_free":true,"livebench_coding":62.8,"livecodebench":52.5,"mbpp_plus":80.1,"model_id":"tngtech/deepseek-r1t2-chimera:free","model_name":"TNG: DeepSeek R1T2 Chimera (free)","openrouter_id":"tngtech/deepseek-r1t2-chimera:free","price_per_million_input":0,"price_per_million_output":0,"provider":"tngtech","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":44.8,"swe_bench_verified":49.2},{"bigcodebench_full":70.5,"bigcodebench_hard":58.8,"composite_score":68.87,"context_length":163840,"description":"DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI\u2019s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2\u00d7 faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.","huggingface_id":"tngtech/DeepSeek-TNG-R1T2-Chimera","humaneval_plus":93.5,"is_free":false,"livebench_coding":62.8,"livecodebench":52.5,"mbpp_plus":80.1,"model_id":"tngtech/deepseek-r1t2-chimera","model_name":"TNG: DeepSeek R1T2 Chimera","openrouter_id":"tngtech/deepseek-r1t2-chimera","price_per_million_input":0.3,"price_per_million_output":1.2,"provider":"tngtech","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":44.8,"swe_bench_verified":49.2},{"bigcodebench_full":70.5,"bigcodebench_hard":58.8,"composite_score":68.87,"context_length":32768,"description":"DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter post-training tricks, pushing its reasoning and inference to the brink of flagship models like O3 and Gemini 2.5 Pro.\nIt now tops math, programming, and logic leaderboards, showcasing a step-change in depth-of-thought.\nThe distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers this chain-of-thought into an 8 B-parameter form, beating standard Qwen3 8B by +10 pp and tying the 235 B \u201cthinking\u201d giant on AIME 2024.","huggingface_id":"deepseek-ai/deepseek-r1-0528-qwen3-8b","humaneval_plus":93.5,"is_free":false,"livebench_coding":62.8,"livecodebench":52.5,"mbpp_plus":80.1,"model_id":"deepseek/deepseek-r1-0528-qwen3-8b","model_name":"DeepSeek: DeepSeek R1 0528 Qwen3 8B","openrouter_id":"deepseek/deepseek-r1-0528-qwen3-8b","price_per_million_input":0.02,"price_per_million_output":0.09999999999999999,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":44.8,"swe_bench_verified":49.2},{"bigcodebench_full":70.5,"bigcodebench_hard":58.8,"composite_score":68.87,"context_length":163840,"description":"May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.","huggingface_id":"deepseek-ai/DeepSeek-R1-0528","humaneval_plus":93.5,"is_free":false,"livebench_coding":62.8,"livecodebench":52.5,"mbpp_plus":80.1,"model_id":"deepseek/deepseek-r1-0528","model_name":"DeepSeek: R1 0528","openrouter_id":"deepseek/deepseek-r1-0528","price_per_million_input":0.39999999999999997,"price_per_million_output":1.75,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":44.8,"swe_bench_verified":49.2},{"bigcodebench_full":70.5,"bigcodebench_hard":58.8,"composite_score":68.87,"context_length":163840,"description":"DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks.\n\nThe model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.","huggingface_id":"tngtech/DeepSeek-R1T-Chimera","humaneval_plus":93.5,"is_free":true,"livebench_coding":62.8,"livecodebench":52.5,"mbpp_plus":80.1,"model_id":"tngtech/deepseek-r1t-chimera:free","model_name":"TNG: DeepSeek R1T Chimera (free)","openrouter_id":"tngtech/deepseek-r1t-chimera:free","price_per_million_input":0,"price_per_million_output":0,"provider":"tngtech","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":44.8,"swe_bench_verified":49.2},{"bigcodebench_full":70.5,"bigcodebench_hard":58.8,"composite_score":68.87,"context_length":163840,"description":"DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks.\n\nThe model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.","huggingface_id":"tngtech/DeepSeek-R1T-Chimera","humaneval_plus":93.5,"is_free":false,"livebench_coding":62.8,"livecodebench":52.5,"mbpp_plus":80.1,"model_id":"tngtech/deepseek-r1t-chimera","model_name":"TNG: DeepSeek R1T Chimera","openrouter_id":"tngtech/deepseek-r1t-chimera","price_per_million_input":0.3,"price_per_million_output":1.2,"provider":"tngtech","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":44.8,"swe_bench_verified":49.2},{"bigcodebench_full":70.5,"bigcodebench_hard":58.8,"composite_score":68.87,"context_length":163840,"description":"DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model & [technical report](https://api-docs.deepseek.com/news/news250120).\n\nMIT licensed: Distill & commercialize freely!","huggingface_id":"deepseek-ai/DeepSeek-R1","humaneval_plus":93.5,"is_free":false,"livebench_coding":62.8,"livecodebench":52.5,"mbpp_plus":80.1,"model_id":"deepseek/deepseek-r1","model_name":"DeepSeek: R1","openrouter_id":"deepseek/deepseek-r1","price_per_million_input":0.3,"price_per_million_output":1.2,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":44.8,"swe_bench_verified":49.2},{"bigcodebench_full":68.2,"bigcodebench_hard":56.8,"composite_score":67.1,"context_length":200000,"description":"New Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Scores ~49% on SWE-Bench Verified, higher than the last best score, and without any fancy prompt scaffolding\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\n#multimodal","huggingface_id":null,"humaneval_plus":92.0,"is_free":false,"livebench_coding":58.2,"livecodebench":38.5,"mbpp_plus":78.4,"model_id":"anthropic/claude-3.5-sonnet","model_name":"Anthropic: Claude 3.5 Sonnet","openrouter_id":"anthropic/claude-3.5-sonnet","price_per_million_input":6.0,"price_per_million_output":30.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":43.6,"swe_bench_verified":49.0},{"bigcodebench_full":70.2,"bigcodebench_hard":58.4,"composite_score":65.21,"context_length":200000,"description":"The o1 series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o1-pro model uses more compute to think harder and provide consistently better answers.","huggingface_id":"","humaneval_plus":88.1,"is_free":false,"livebench_coding":62.5,"livecodebench":48.5,"mbpp_plus":79.2,"model_id":"openai/o1-pro","model_name":"OpenAI: o1-pro","openrouter_id":"openai/o1-pro","price_per_million_input":150.0,"price_per_million_output":600.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":36.5,"swe_bench_verified":41.0},{"bigcodebench_full":70.2,"bigcodebench_hard":58.4,"composite_score":65.21,"context_length":200000,"description":"The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding. The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. \n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n","huggingface_id":"","humaneval_plus":88.1,"is_free":false,"livebench_coding":62.5,"livecodebench":48.5,"mbpp_plus":79.2,"model_id":"openai/o1","model_name":"OpenAI: o1","openrouter_id":"openai/o1","price_per_million_input":15.0,"price_per_million_output":60.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":36.5,"swe_bench_verified":41.0},{"bigcodebench_full":67.8,"bigcodebench_hard":55.4,"composite_score":65.09,"context_length":131072,"description":"DeepSeek V3.1 Nex-N1 is the flagship release of the Nex-N1 series \u2014 a post-trained model designed to highlight agent autonomy, tool use, and real-world productivity. \n\nNex-N1 demonstrates competitive performance across all evaluation scenarios, showing particularly strong results in practical coding and HTML generation tasks.","huggingface_id":"nex-agi/DeepSeek-V3.1-Nex-N1","humaneval_plus":92.1,"is_free":true,"livebench_coding":58.4,"livecodebench":42.8,"mbpp_plus":78.2,"model_id":"nex-agi/deepseek-v3.1-nex-n1:free","model_name":"Nex AGI: DeepSeek V3.1 Nex N1 (free)","openrouter_id":"nex-agi/deepseek-v3.1-nex-n1:free","price_per_million_input":0,"price_per_million_output":0,"provider":"nex-agi","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":37.5,"swe_bench_verified":42.0},{"bigcodebench_full":67.8,"bigcodebench_hard":55.4,"composite_score":65.09,"context_length":163840,"description":"DeepSeek-V3.2-Speciale is a high-compute variant of DeepSeek-V3.2 optimized for maximum reasoning and agentic performance. It builds on DeepSeek Sparse Attention (DSA) for efficient long-context processing, then scales post-training reinforcement learning to push capability beyond the base model. Reported evaluations place Speciale ahead of GPT-5 on difficult reasoning workloads, with proficiency comparable to Gemini-3.0-Pro, while retaining strong coding and tool-use reliability. Like V3.2, it benefits from a large-scale agentic task synthesis pipeline that improves compliance and generalization in interactive environments.","huggingface_id":"deepseek-ai/DeepSeek-V3.2-Speciale","humaneval_plus":92.1,"is_free":false,"livebench_coding":58.4,"livecodebench":42.8,"mbpp_plus":78.2,"model_id":"deepseek/deepseek-v3.2-speciale","model_name":"DeepSeek: DeepSeek V3.2 Speciale","openrouter_id":"deepseek/deepseek-v3.2-speciale","price_per_million_input":0.27,"price_per_million_output":0.41,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":37.5,"swe_bench_verified":42.0},{"bigcodebench_full":67.8,"bigcodebench_hard":55.4,"composite_score":65.09,"context_length":163840,"description":"DeepSeek-V3.2 is a large language model designed to harmonize high computational efficiency with strong reasoning and agentic tool-use performance. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism that reduces training and inference cost while preserving quality in long-context scenarios. A scalable reinforcement learning post-training framework further improves reasoning, with reported performance in the GPT-5 class, and the model has demonstrated gold-medal results on the 2025 IMO and IOI. V3.2 also uses a large-scale agentic task synthesis pipeline to better integrate reasoning into tool-use settings, boosting compliance and generalization in interactive environments.\n\nUsers can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)","huggingface_id":"deepseek-ai/DeepSeek-V3.2","humaneval_plus":92.1,"is_free":false,"livebench_coding":58.4,"livecodebench":42.8,"mbpp_plus":78.2,"model_id":"deepseek/deepseek-v3.2","model_name":"DeepSeek: DeepSeek V3.2","openrouter_id":"deepseek/deepseek-v3.2","price_per_million_input":0.25,"price_per_million_output":0.38,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":37.5,"swe_bench_verified":42.0},{"bigcodebench_full":67.8,"bigcodebench_hard":55.4,"composite_score":65.09,"context_length":163840,"description":"DeepSeek-V3.2-Exp is an experimental large language model released by DeepSeek as an intermediate step between V3.1 and future architectures. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism designed to improve training and inference efficiency in long-context scenarios while maintaining output quality. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model was trained under conditions aligned with V3.1-Terminus to enable direct comparison. Benchmarking shows performance roughly on par with V3.1 across reasoning, coding, and agentic tool-use tasks, with minor tradeoffs and gains depending on the domain. This release focuses on validating architectural optimizations for extended context lengths rather than advancing raw task accuracy, making it primarily a research-oriented model for exploring efficient transformer designs.","huggingface_id":"deepseek-ai/DeepSeek-V3.2-Exp","humaneval_plus":92.1,"is_free":false,"livebench_coding":58.4,"livecodebench":42.8,"mbpp_plus":78.2,"model_id":"deepseek/deepseek-v3.2-exp","model_name":"DeepSeek: DeepSeek V3.2 Exp","openrouter_id":"deepseek/deepseek-v3.2-exp","price_per_million_input":0.21,"price_per_million_output":0.32,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":37.5,"swe_bench_verified":42.0},{"bigcodebench_full":67.8,"bigcodebench_hard":55.4,"composite_score":65.09,"context_length":163840,"description":"DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. ","huggingface_id":"deepseek-ai/DeepSeek-V3.1-Terminus","humaneval_plus":92.1,"is_free":false,"livebench_coding":58.4,"livecodebench":42.8,"mbpp_plus":78.2,"model_id":"deepseek/deepseek-v3.1-terminus:exacto","model_name":"DeepSeek: DeepSeek V3.1 Terminus (exacto)","openrouter_id":"deepseek/deepseek-v3.1-terminus:exacto","price_per_million_input":0.21,"price_per_million_output":0.7899999999999999,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":37.5,"swe_bench_verified":42.0},{"bigcodebench_full":67.8,"bigcodebench_hard":55.4,"composite_score":65.09,"context_length":163840,"description":"DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. ","huggingface_id":"deepseek-ai/DeepSeek-V3.1-Terminus","humaneval_plus":92.1,"is_free":false,"livebench_coding":58.4,"livecodebench":42.8,"mbpp_plus":78.2,"model_id":"deepseek/deepseek-v3.1-terminus","model_name":"DeepSeek: DeepSeek V3.1 Terminus","openrouter_id":"deepseek/deepseek-v3.1-terminus","price_per_million_input":0.21,"price_per_million_output":0.7899999999999999,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":37.5,"swe_bench_verified":42.0},{"bigcodebench_full":65.4,"bigcodebench_hard":52.8,"composite_score":64.37,"context_length":32768,"description":"Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n\n- Significantly improvements in **code generation**, **code reasoning** and **code fixing**. \n- A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n\nTo read more about its evaluation results, check out [Qwen 2.5 Coder's blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).","huggingface_id":"Qwen/Qwen2.5-Coder-32B-Instruct","humaneval_plus":92.7,"is_free":false,"livebench_coding":55.8,"livecodebench":45.8,"mbpp_plus":79.1,"model_id":"qwen/qwen-2.5-coder-32b-instruct","model_name":"Qwen2.5 Coder 32B Instruct","openrouter_id":"qwen/qwen-2.5-coder-32b-instruct","price_per_million_input":0.03,"price_per_million_output":0.11,"provider":"qwen","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":36.8,"swe_bench_verified":41.6},{"bigcodebench_full":61.2,"bigcodebench_hard":48.5,"composite_score":64.03,"context_length":32768,"description":"QwQ-32B-ArliAI-RpR-v1 is a 32B parameter model fine-tuned from Qwen/QwQ-32B using a curated creative writing and roleplay dataset originally developed for the RPMax series. It is designed to maintain coherence and reasoning across long multi-turn conversations by introducing explicit reasoning steps per dialogue turn, generated and refined using the base model itself.\n\nThe model was trained using RS-QLORA+ on 8K sequence lengths and supports up to 128K context windows (with practical performance around 32K). It is optimized for creative roleplay and dialogue generation, with an emphasis on minimizing cross-context repetition while preserving stylistic diversity.","huggingface_id":"ArliAI/QwQ-32B-ArliAI-RpR-v1","humaneval_plus":90.5,"is_free":false,"livebench_coding":58.2,"livecodebench":52.8,"mbpp_plus":77.8,"model_id":"arliai/qwq-32b-arliai-rpr-v1","model_name":"ArliAI: QwQ 32B RpR v1","openrouter_id":"arliai/qwq-32b-arliai-rpr-v1","price_per_million_input":0.03,"price_per_million_output":0.11,"provider":"arliai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":40.5,"swe_bench_verified":45.2},{"bigcodebench_full":61.2,"bigcodebench_hard":48.5,"composite_score":64.03,"context_length":32768,"description":"QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.","huggingface_id":"Qwen/QwQ-32B","humaneval_plus":90.5,"is_free":false,"livebench_coding":58.2,"livecodebench":52.8,"mbpp_plus":77.8,"model_id":"qwen/qwq-32b","model_name":"Qwen: QwQ 32B","openrouter_id":"qwen/qwq-32b","price_per_million_input":0.15,"price_per_million_output":0.39999999999999997,"provider":"qwen","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":40.5,"swe_bench_verified":45.2},{"bigcodebench_full":67.2,"bigcodebench_hard":54.5,"composite_score":63.15,"context_length":1047576,"description":"GPT-4.1 is a flagship large language model optimized for advanced instruction following, real-world software engineering, and long-context reasoning. It supports a 1 million token context window and outperforms GPT-4o and GPT-4.5 across coding (54.6% SWE-bench Verified), instruction compliance (87.4% IFEval), and multimodal understanding benchmarks. It is tuned for precise code diffs, agent reliability, and high recall in large document contexts, making it ideal for agents, IDE tooling, and enterprise knowledge retrieval.","huggingface_id":"","humaneval_plus":91.5,"is_free":false,"livebench_coding":56.8,"livecodebench":35.8,"mbpp_plus":78.2,"model_id":"openai/gpt-4.1","model_name":"OpenAI: GPT-4.1","openrouter_id":"openai/gpt-4.1","price_per_million_input":2.0,"price_per_million_output":8.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":31.8,"swe_bench_verified":36.5},{"bigcodebench_full":67.2,"bigcodebench_hard":54.5,"composite_score":63.15,"context_length":1047576,"description":"GPT-4.1 Mini is a mid-sized model delivering performance competitive with GPT-4o at substantially lower latency and cost. It retains a 1 million token context window and scores 45.1% on hard instruction evals, 35.8% on MultiChallenge, and 84.1% on IFEval. Mini also shows strong coding ability (e.g., 31.6% on Aider\u2019s polyglot diff benchmark) and vision understanding, making it suitable for interactive applications with tight performance constraints.","huggingface_id":"","humaneval_plus":91.5,"is_free":false,"livebench_coding":56.8,"livecodebench":35.8,"mbpp_plus":78.2,"model_id":"openai/gpt-4.1-mini","model_name":"OpenAI: GPT-4.1 Mini","openrouter_id":"openai/gpt-4.1-mini","price_per_million_input":0.39999999999999997,"price_per_million_output":1.5999999999999999,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":31.8,"swe_bench_verified":36.5},{"bigcodebench_full":67.2,"bigcodebench_hard":54.5,"composite_score":63.15,"context_length":1047576,"description":"For tasks that demand low latency, GPT\u20114.1 nano is the fastest and cheapest model in the GPT-4.1 series. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot coding \u2013 even higher than GPT\u20114o mini. It\u2019s ideal for tasks like classification or autocompletion.","huggingface_id":"","humaneval_plus":91.5,"is_free":false,"livebench_coding":56.8,"livecodebench":35.8,"mbpp_plus":78.2,"model_id":"openai/gpt-4.1-nano","model_name":"OpenAI: GPT-4.1 Nano","openrouter_id":"openai/gpt-4.1-nano","price_per_million_input":0.09999999999999999,"price_per_million_output":0.39999999999999997,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":31.8,"swe_bench_verified":36.5},{"bigcodebench_full":63.8,"bigcodebench_hard":50.2,"composite_score":62.39,"context_length":32768,"description":"Gemini 2.5 Flash Image, a.k.a. \"Nano Banana,\" is now generally available. It is a state of the art image generation model with contextual understanding. It is capable of image generation, edits, and multi-turn conversations. Aspect ratios can be controlled with the [image_config API Parameter](https://openrouter.ai/docs/features/multimodal/image-generation#image-aspect-ratio-configuration)","huggingface_id":"","humaneval_plus":90.8,"is_free":false,"livebench_coding":55.8,"livecodebench":38.5,"mbpp_plus":77.2,"model_id":"google/gemini-2.5-flash-image","model_name":"Google: Gemini 2.5 Flash Image (Nano Banana)","openrouter_id":"google/gemini-2.5-flash-image","price_per_million_input":0.3,"price_per_million_output":2.5,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":34.2,"swe_bench_verified":38.8},{"bigcodebench_full":63.8,"bigcodebench_hard":50.2,"composite_score":62.39,"context_length":1048576,"description":"Gemini 2.5 Flash Preview September 2025 Checkpoint is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).","huggingface_id":"","humaneval_plus":90.8,"is_free":false,"livebench_coding":55.8,"livecodebench":38.5,"mbpp_plus":77.2,"model_id":"google/gemini-2.5-flash-preview-09-2025","model_name":"Google: Gemini 2.5 Flash Preview 09-2025","openrouter_id":"google/gemini-2.5-flash-preview-09-2025","price_per_million_input":0.3,"price_per_million_output":2.5,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":34.2,"swe_bench_verified":38.8},{"bigcodebench_full":63.8,"bigcodebench_hard":50.2,"composite_score":62.39,"context_length":1048576,"description":"Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ","huggingface_id":"","humaneval_plus":90.8,"is_free":false,"livebench_coding":55.8,"livecodebench":38.5,"mbpp_plus":77.2,"model_id":"google/gemini-2.5-flash-lite-preview-09-2025","model_name":"Google: Gemini 2.5 Flash Lite Preview 09-2025","openrouter_id":"google/gemini-2.5-flash-lite-preview-09-2025","price_per_million_input":0.09999999999999999,"price_per_million_output":0.39999999999999997,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":34.2,"swe_bench_verified":38.8},{"bigcodebench_full":63.8,"bigcodebench_hard":50.2,"composite_score":62.39,"context_length":32768,"description":"Gemini 2.5 Flash Image Preview, a.k.a. \"Nano Banana,\" is a state of the art image generation model with contextual understanding. It is capable of image generation, edits, and multi-turn conversations.","huggingface_id":"","humaneval_plus":90.8,"is_free":false,"livebench_coding":55.8,"livecodebench":38.5,"mbpp_plus":77.2,"model_id":"google/gemini-2.5-flash-image-preview","model_name":"Google: Gemini 2.5 Flash Image Preview (Nano Banana)","openrouter_id":"google/gemini-2.5-flash-image-preview","price_per_million_input":0.3,"price_per_million_output":2.5,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":34.2,"swe_bench_verified":38.8},{"bigcodebench_full":63.8,"bigcodebench_hard":50.2,"composite_score":62.39,"context_length":1048576,"description":"Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ","huggingface_id":"","humaneval_plus":90.8,"is_free":false,"livebench_coding":55.8,"livecodebench":38.5,"mbpp_plus":77.2,"model_id":"google/gemini-2.5-flash-lite","model_name":"Google: Gemini 2.5 Flash Lite","openrouter_id":"google/gemini-2.5-flash-lite","price_per_million_input":0.09999999999999999,"price_per_million_output":0.39999999999999997,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":34.2,"swe_bench_verified":38.8},{"bigcodebench_full":63.8,"bigcodebench_hard":50.2,"composite_score":62.39,"context_length":1048576,"description":"Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).","huggingface_id":"","humaneval_plus":90.8,"is_free":false,"livebench_coding":55.8,"livecodebench":38.5,"mbpp_plus":77.2,"model_id":"google/gemini-2.5-flash","model_name":"Google: Gemini 2.5 Flash","openrouter_id":"google/gemini-2.5-flash","price_per_million_input":0.3,"price_per_million_output":2.5,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":34.2,"swe_bench_verified":38.8},{"bigcodebench_full":61.2,"bigcodebench_hard":48.5,"composite_score":61.81,"context_length":131072,"description":"DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including:\n\n- AIME 2024 pass@1: 70.0\n- MATH-500 pass@1: 94.5\n- CodeForces Rating: 1633\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.","huggingface_id":"deepseek-ai/DeepSeek-R1-Distill-Llama-70B","humaneval_plus":90.2,"is_free":false,"livebench_coding":52.8,"livecodebench":40.5,"mbpp_plus":76.8,"model_id":"deepseek/deepseek-r1-distill-llama-70b","model_name":"DeepSeek: R1 Distill Llama 70B","openrouter_id":"deepseek/deepseek-r1-distill-llama-70b","price_per_million_input":0.03,"price_per_million_output":0.13,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":35.8,"swe_bench_verified":40.5},{"bigcodebench_full":60.5,"bigcodebench_hard":48.2,"composite_score":61.07,"context_length":8192,"description":"DeepSeek-V3.1 is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes via prompt templates. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. \n\nIt succeeds the [DeepSeek V3-0324](/deepseek/deepseek-chat-v3-0324) model and performs well on a variety of tasks.","huggingface_id":"deepseek-ai/DeepSeek-V3.1","humaneval_plus":89.8,"is_free":false,"livebench_coding":52.5,"livecodebench":36.5,"mbpp_plus":76.5,"model_id":"deepseek/deepseek-chat-v3.1","model_name":"DeepSeek: DeepSeek V3.1","openrouter_id":"deepseek/deepseek-chat-v3.1","price_per_million_input":0.15,"price_per_million_output":0.75,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":33.8,"swe_bench_verified":38.5},{"bigcodebench_full":60.5,"bigcodebench_hard":48.2,"composite_score":61.07,"context_length":163840,"description":"DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.","huggingface_id":"deepseek-ai/DeepSeek-V3-0324","humaneval_plus":89.8,"is_free":false,"livebench_coding":52.5,"livecodebench":36.5,"mbpp_plus":76.5,"model_id":"deepseek/deepseek-chat-v3-0324","model_name":"DeepSeek: DeepSeek V3 0324","openrouter_id":"deepseek/deepseek-chat-v3-0324","price_per_million_input":0.19999999999999998,"price_per_million_output":0.88,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":33.8,"swe_bench_verified":38.5},{"bigcodebench_full":60.5,"bigcodebench_hard":48.2,"composite_score":61.07,"context_length":163840,"description":"DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.\n\nFor model details, please visit [the DeepSeek-V3 repo](https://github.com/deepseek-ai/DeepSeek-V3) for more information, or see the [launch announcement](https://api-docs.deepseek.com/news/news1226).","huggingface_id":"deepseek-ai/DeepSeek-V3","humaneval_plus":89.8,"is_free":false,"livebench_coding":52.5,"livecodebench":36.5,"mbpp_plus":76.5,"model_id":"deepseek/deepseek-chat","model_name":"DeepSeek: DeepSeek V3","openrouter_id":"deepseek/deepseek-chat","price_per_million_input":0.3,"price_per_million_output":1.2,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":33.8,"swe_bench_verified":38.5},{"bigcodebench_full":64.5,"bigcodebench_hard":51.2,"composite_score":60.38,"context_length":128000,"description":"The gpt-4o-audio-preview model adds support for audio inputs as prompts. This enhancement allows the model to detect nuances within audio recordings and add depth to generated user experiences. Audio outputs are currently not supported. Audio tokens are priced at $40 per million input audio tokens.","huggingface_id":"","humaneval_plus":90.2,"is_free":false,"livebench_coding":52.4,"livecodebench":32.5,"mbpp_plus":76.2,"model_id":"openai/gpt-4o-audio-preview","model_name":"OpenAI: GPT-4o Audio","openrouter_id":"openai/gpt-4o-audio-preview","price_per_million_input":2.5,"price_per_million_output":10.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.5,"swe_bench_verified":33.2},{"bigcodebench_full":64.5,"bigcodebench_hard":51.2,"composite_score":60.38,"context_length":128000,"description":"GPT-4o Search Previewis a specialized model for web search in Chat Completions. It is trained to understand and execute web search queries.","huggingface_id":"","humaneval_plus":90.2,"is_free":false,"livebench_coding":52.4,"livecodebench":32.5,"mbpp_plus":76.2,"model_id":"openai/gpt-4o-search-preview","model_name":"OpenAI: GPT-4o Search Preview","openrouter_id":"openai/gpt-4o-search-preview","price_per_million_input":2.5,"price_per_million_output":10.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.5,"swe_bench_verified":33.2},{"bigcodebench_full":64.5,"bigcodebench_hard":51.2,"composite_score":60.38,"context_length":128000,"description":"The 2024-11-20 version of GPT-4o offers a leveled-up creative writing ability with more natural, engaging, and tailored writing to improve relevance & readability. It\u2019s also better at working with uploaded files, providing deeper insights & more thorough responses.\n\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.","huggingface_id":"","humaneval_plus":90.2,"is_free":false,"livebench_coding":52.4,"livecodebench":32.5,"mbpp_plus":76.2,"model_id":"openai/gpt-4o-2024-11-20","model_name":"OpenAI: GPT-4o (2024-11-20)","openrouter_id":"openai/gpt-4o-2024-11-20","price_per_million_input":2.5,"price_per_million_output":10.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.5,"swe_bench_verified":33.2},{"bigcodebench_full":64.5,"bigcodebench_hard":51.2,"composite_score":60.38,"context_length":128000,"description":"OpenAI ChatGPT 4o is continually updated by OpenAI to point to the current version of GPT-4o used by ChatGPT. It therefore differs slightly from the API version of [GPT-4o](/models/openai/gpt-4o) in that it has additional RLHF. It is intended for research and evaluation.\n\nOpenAI notes that this model is not suited for production use-cases as it may be removed or redirected to another model in the future.","huggingface_id":null,"humaneval_plus":90.2,"is_free":false,"livebench_coding":52.4,"livecodebench":32.5,"mbpp_plus":76.2,"model_id":"openai/chatgpt-4o-latest","model_name":"OpenAI: ChatGPT-4o","openrouter_id":"openai/chatgpt-4o-latest","price_per_million_input":5.0,"price_per_million_output":15.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.5,"swe_bench_verified":33.2},{"bigcodebench_full":64.5,"bigcodebench_hard":51.2,"composite_score":60.38,"context_length":128000,"description":"The 2024-08-06 version of GPT-4o offers improved performance in structured outputs, with the ability to supply a JSON schema in the respone_format. Read more [here](https://openai.com/index/introducing-structured-outputs-in-the-api/).\n\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)","huggingface_id":null,"humaneval_plus":90.2,"is_free":false,"livebench_coding":52.4,"livecodebench":32.5,"mbpp_plus":76.2,"model_id":"openai/gpt-4o-2024-08-06","model_name":"OpenAI: GPT-4o (2024-08-06)","openrouter_id":"openai/gpt-4o-2024-08-06","price_per_million_input":2.5,"price_per_million_output":10.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.5,"swe_bench_verified":33.2},{"bigcodebench_full":64.5,"bigcodebench_hard":51.2,"composite_score":60.38,"context_length":128000,"description":"GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal","huggingface_id":null,"humaneval_plus":90.2,"is_free":false,"livebench_coding":52.4,"livecodebench":32.5,"mbpp_plus":76.2,"model_id":"openai/gpt-4o-2024-05-13","model_name":"OpenAI: GPT-4o (2024-05-13)","openrouter_id":"openai/gpt-4o-2024-05-13","price_per_million_input":5.0,"price_per_million_output":15.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.5,"swe_bench_verified":33.2},{"bigcodebench_full":64.5,"bigcodebench_hard":51.2,"composite_score":60.38,"context_length":128000,"description":"GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal","huggingface_id":null,"humaneval_plus":90.2,"is_free":false,"livebench_coding":52.4,"livecodebench":32.5,"mbpp_plus":76.2,"model_id":"openai/gpt-4o","model_name":"OpenAI: GPT-4o","openrouter_id":"openai/gpt-4o","price_per_million_input":2.5,"price_per_million_output":10.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.5,"swe_bench_verified":33.2},{"bigcodebench_full":64.5,"bigcodebench_hard":51.2,"composite_score":60.38,"context_length":128000,"description":"GPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n#multimodal","huggingface_id":null,"humaneval_plus":90.2,"is_free":false,"livebench_coding":52.4,"livecodebench":32.5,"mbpp_plus":76.2,"model_id":"openai/gpt-4o:extended","model_name":"OpenAI: GPT-4o (extended)","openrouter_id":"openai/gpt-4o:extended","price_per_million_input":6.0,"price_per_million_output":18.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.5,"swe_bench_verified":33.2},{"bigcodebench_full":61.2,"bigcodebench_hard":48.5,"composite_score":60.24,"context_length":1048576,"description":"Gemini 2.0 Flash Lite offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5), all at extremely economical token prices.","huggingface_id":"","humaneval_plus":89.1,"is_free":false,"livebench_coding":54.5,"livecodebench":35.8,"mbpp_plus":75.3,"model_id":"google/gemini-2.0-flash-lite-001","model_name":"Google: Gemini 2.0 Flash Lite","openrouter_id":"google/gemini-2.0-flash-lite-001","price_per_million_input":0.075,"price_per_million_output":0.3,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":30.4,"swe_bench_verified":35.2},{"bigcodebench_full":61.2,"bigcodebench_hard":48.5,"composite_score":60.24,"context_length":1048576,"description":"Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.","huggingface_id":"","humaneval_plus":89.1,"is_free":false,"livebench_coding":54.5,"livecodebench":35.8,"mbpp_plus":75.3,"model_id":"google/gemini-2.0-flash-001","model_name":"Google: Gemini 2.0 Flash","openrouter_id":"google/gemini-2.0-flash-001","price_per_million_input":0.09999999999999999,"price_per_million_output":0.39999999999999997,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":30.4,"swe_bench_verified":35.2},{"bigcodebench_full":61.2,"bigcodebench_hard":48.5,"composite_score":60.24,"context_length":1048576,"description":"Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.","huggingface_id":"","humaneval_plus":89.1,"is_free":true,"livebench_coding":54.5,"livecodebench":35.8,"mbpp_plus":75.3,"model_id":"google/gemini-2.0-flash-exp:free","model_name":"Google: Gemini 2.0 Flash Experimental (free)","openrouter_id":"google/gemini-2.0-flash-exp:free","price_per_million_input":0,"price_per_million_output":0,"provider":"google","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":30.4,"swe_bench_verified":35.2},{"bigcodebench_full":58.8,"bigcodebench_hard":46.2,"composite_score":60.12,"context_length":64000,"description":"DeepSeek R1 Distill Qwen 32B is a distilled large language model based on [Qwen 2.5 32B](https://huggingface.co/Qwen/Qwen2.5-32B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\n\\nOther benchmark results include:\\n\\n- AIME 2024 pass@1: 72.6\\n- MATH-500 pass@1: 94.3\\n- CodeForces Rating: 1691\\n\\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.","huggingface_id":"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B","humaneval_plus":89.5,"is_free":false,"livebench_coding":50.5,"livecodebench":38.2,"mbpp_plus":75.9,"model_id":"deepseek/deepseek-r1-distill-qwen-32b","model_name":"DeepSeek: R1 Distill Qwen 32B","openrouter_id":"deepseek/deepseek-r1-distill-qwen-32b","price_per_million_input":0.24,"price_per_million_output":0.24,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":33.5,"swe_bench_verified":38.2},{"bigcodebench_full":58.4,"bigcodebench_hard":45.2,"composite_score":59.34,"context_length":200000,"description":"Claude 3.5 Haiku features enhancements across all skill sets including coding, tool use, and reasoning. As the fastest model in the Anthropic lineup, it offers rapid response times suitable for applications that require high interactivity and low latency, such as user-facing chatbots and on-the-fly code completions. It also excels in specialized tasks like data extraction and real-time content moderation, making it a versatile tool for a broad range of industries.\n\nIt does not support image inputs.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/3-5-models-and-computer-use)","huggingface_id":null,"humaneval_plus":88.6,"is_free":false,"livebench_coding":48.5,"livecodebench":28.5,"mbpp_plus":75.2,"model_id":"anthropic/claude-3.5-haiku-20241022","model_name":"Anthropic: Claude 3.5 Haiku (2024-10-22)","openrouter_id":"anthropic/claude-3.5-haiku-20241022","price_per_million_input":0.7999999999999999,"price_per_million_output":4.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":33.8,"swe_bench_verified":38.4},{"bigcodebench_full":58.4,"bigcodebench_hard":45.2,"composite_score":59.34,"context_length":200000,"description":"Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\n\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\n\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).","huggingface_id":null,"humaneval_plus":88.6,"is_free":false,"livebench_coding":48.5,"livecodebench":28.5,"mbpp_plus":75.2,"model_id":"anthropic/claude-3.5-haiku","model_name":"Anthropic: Claude 3.5 Haiku","openrouter_id":"anthropic/claude-3.5-haiku","price_per_million_input":0.7999999999999999,"price_per_million_output":4.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":33.8,"swe_bench_verified":38.4},{"bigcodebench_full":64.5,"bigcodebench_hard":51.2,"composite_score":58.97,"context_length":128000,"description":"The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling.\n\nTraining data: up to April 2023.","huggingface_id":null,"humaneval_plus":86.6,"is_free":false,"livebench_coding":52.4,"livecodebench":32.5,"mbpp_plus":72.8,"model_id":"openai/gpt-4-1106-preview","model_name":"OpenAI: GPT-4 Turbo (older v1106)","openrouter_id":"openai/gpt-4-1106-preview","price_per_million_input":10.0,"price_per_million_output":30.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.5,"swe_bench_verified":33.2},{"bigcodebench_full":64.5,"bigcodebench_hard":51.2,"composite_score":58.97,"context_length":8191,"description":"OpenAI's flagship model, GPT-4 is a large-scale multimodal language model capable of solving difficult problems with greater accuracy than previous models due to its broader general knowledge and advanced reasoning capabilities. Training data: up to Sep 2021.","huggingface_id":null,"humaneval_plus":86.6,"is_free":false,"livebench_coding":52.4,"livecodebench":32.5,"mbpp_plus":72.8,"model_id":"openai/gpt-4","model_name":"OpenAI: GPT-4","openrouter_id":"openai/gpt-4","price_per_million_input":30.0,"price_per_million_output":60.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.5,"swe_bench_verified":33.2},{"bigcodebench_full":64.5,"bigcodebench_hard":51.2,"composite_score":58.97,"context_length":8191,"description":"GPT-4-0314 is the first version of GPT-4 released, with a context length of 8,192 tokens, and was supported until June 14. Training data: up to Sep 2021.","huggingface_id":null,"humaneval_plus":86.6,"is_free":false,"livebench_coding":52.4,"livecodebench":32.5,"mbpp_plus":72.8,"model_id":"openai/gpt-4-0314","model_name":"OpenAI: GPT-4 (older v0314)","openrouter_id":"openai/gpt-4-0314","price_per_million_input":30.0,"price_per_million_output":60.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.5,"swe_bench_verified":33.2},{"bigcodebench_full":61.8,"bigcodebench_hard":48.5,"composite_score":56.62,"context_length":128000,"description":"The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling.\n\nTraining data: up to December 2023.","huggingface_id":null,"humaneval_plus":87.1,"is_free":false,"livebench_coding":48.5,"livecodebench":28.2,"mbpp_plus":74.5,"model_id":"openai/gpt-4-turbo","model_name":"OpenAI: GPT-4 Turbo","openrouter_id":"openai/gpt-4-turbo","price_per_million_input":10.0,"price_per_million_output":30.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":23.2,"swe_bench_verified":26.8},{"bigcodebench_full":61.8,"bigcodebench_hard":48.5,"composite_score":56.62,"context_length":128000,"description":"The preview GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Training data: up to Dec 2023.\n\n**Note:** heavily rate limited by OpenAI while in preview.","huggingface_id":null,"humaneval_plus":87.1,"is_free":false,"livebench_coding":48.5,"livecodebench":28.2,"mbpp_plus":74.5,"model_id":"openai/gpt-4-turbo-preview","model_name":"OpenAI: GPT-4 Turbo Preview","openrouter_id":"openai/gpt-4-turbo-preview","price_per_million_input":10.0,"price_per_million_output":30.0,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":23.2,"swe_bench_verified":26.8},{"bigcodebench_full":52.4,"bigcodebench_hard":40.5,"composite_score":55.76,"context_length":32768,"description":"DeepSeek R1 Distill Qwen 14B is a distilled large language model based on [Qwen 2.5 14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\nOther benchmark results include:\n\n- AIME 2024 pass@1: 69.7\n- MATH-500 pass@1: 93.9\n- CodeForces Rating: 1481\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.","huggingface_id":"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B","humaneval_plus":86.8,"is_free":false,"livebench_coding":45.2,"livecodebench":32.8,"mbpp_plus":73.2,"model_id":"deepseek/deepseek-r1-distill-qwen-14b","model_name":"DeepSeek: R1 Distill Qwen 14B","openrouter_id":"deepseek/deepseek-r1-distill-qwen-14b","price_per_million_input":0.12,"price_per_million_output":0.12,"provider":"deepseek","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.4,"swe_bench_verified":32.8},{"bigcodebench_full":62.1,"bigcodebench_hard":48.5,"composite_score":55.27,"context_length":200000,"description":"Claude 3 Opus is Anthropic's most powerful model for highly complex tasks. It boasts top-level performance, intelligence, fluency, and understanding.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal","huggingface_id":null,"humaneval_plus":84.9,"is_free":false,"livebench_coding":52.8,"livecodebench":32.8,"mbpp_plus":72.5,"model_id":"anthropic/claude-3-opus","model_name":"Anthropic: Claude 3 Opus","openrouter_id":"anthropic/claude-3-opus","price_per_million_input":15.0,"price_per_million_output":75.0,"provider":"anthropic","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":18.4,"swe_bench_verified":22.2},{"bigcodebench_full":51.2,"bigcodebench_hard":38.5,"composite_score":54.83,"context_length":32768,"description":"Qwen2.5-Coder-7B-Instruct is a 7B parameter instruction-tuned language model optimized for code-related tasks such as code generation, reasoning, and bug fixing. Based on the Qwen2.5 architecture, it incorporates enhancements like RoPE, SwiGLU, RMSNorm, and GQA attention with support for up to 128K tokens using YaRN-based extrapolation. It is trained on a large corpus of source code, synthetic data, and text-code grounding, providing robust performance across programming languages and agentic coding workflows.\n\nThis model is part of the Qwen2.5-Coder family and offers strong compatibility with tools like vLLM for efficient deployment. Released under the Apache 2.0 license.","huggingface_id":"Qwen/Qwen2.5-Coder-7B-Instruct","humaneval_plus":85.2,"is_free":false,"livebench_coding":45.2,"livecodebench":32.5,"mbpp_plus":72.8,"model_id":"qwen/qwen2.5-coder-7b-instruct","model_name":"Qwen: Qwen2.5 Coder 7B Instruct","openrouter_id":"qwen/qwen2.5-coder-7b-instruct","price_per_million_input":0.03,"price_per_million_output":0.09,"provider":"qwen","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.2,"swe_bench_verified":32.5},{"bigcodebench_full":48.8,"bigcodebench_hard":35.2,"composite_score":54.81,"context_length":131072,"description":"Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\nHermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.","huggingface_id":"NousResearch/Hermes-3-Llama-3.1-405B","humaneval_plus":89.0,"is_free":true,"livebench_coding":50.5,"livecodebench":26.8,"mbpp_plus":75.0,"model_id":"nousresearch/hermes-3-llama-3.1-405b:free","model_name":"Nous: Hermes 3 405B Instruct (free)","openrouter_id":"nousresearch/hermes-3-llama-3.1-405b:free","price_per_million_input":0,"price_per_million_output":0,"provider":"nousresearch","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":22.5,"swe_bench_verified":26.8},{"bigcodebench_full":48.8,"bigcodebench_hard":35.2,"composite_score":54.81,"context_length":131072,"description":"Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\nHermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.","huggingface_id":"NousResearch/Hermes-3-Llama-3.1-405B","humaneval_plus":89.0,"is_free":false,"livebench_coding":50.5,"livecodebench":26.8,"mbpp_plus":75.0,"model_id":"nousresearch/hermes-3-llama-3.1-405b","model_name":"Nous: Hermes 3 405B Instruct","openrouter_id":"nousresearch/hermes-3-llama-3.1-405b","price_per_million_input":1.0,"price_per_million_output":1.0,"provider":"nousresearch","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":22.5,"swe_bench_verified":26.8},{"bigcodebench_full":48.8,"bigcodebench_hard":35.2,"composite_score":54.81,"context_length":32768,"description":"Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This is the base 405B pre-trained version.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).","huggingface_id":"meta-llama/llama-3.1-405B","humaneval_plus":89.0,"is_free":false,"livebench_coding":50.5,"livecodebench":26.8,"mbpp_plus":75.0,"model_id":"meta-llama/llama-3.1-405b","model_name":"Meta: Llama 3.1 405B (base)","openrouter_id":"meta-llama/llama-3.1-405b","price_per_million_input":4.0,"price_per_million_output":4.0,"provider":"meta-llama","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":22.5,"swe_bench_verified":26.8},{"bigcodebench_full":48.8,"bigcodebench_hard":35.2,"composite_score":54.81,"context_length":130815,"description":"The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eval scores, the Meta AI team continues to push the frontier of open-source LLMs.\n\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 405B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models including GPT-4o and Claude 3.5 Sonnet in evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).","huggingface_id":"meta-llama/Meta-Llama-3.1-405B-Instruct","humaneval_plus":89.0,"is_free":false,"livebench_coding":50.5,"livecodebench":26.8,"mbpp_plus":75.0,"model_id":"meta-llama/llama-3.1-405b-instruct","model_name":"Meta: Llama 3.1 405B Instruct","openrouter_id":"meta-llama/llama-3.1-405b-instruct","price_per_million_input":3.5,"price_per_million_output":3.5,"provider":"meta-llama","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":22.5,"swe_bench_verified":26.8},{"bigcodebench_full":42.5,"bigcodebench_hard":28.4,"composite_score":54.31,"context_length":131072,"description":"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n[Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)","huggingface_id":"meta-llama/Llama-3.3-70B-Instruct","humaneval_plus":88.2,"is_free":true,"livebench_coding":48.2,"livecodebench":28.5,"mbpp_plus":74.8,"model_id":"meta-llama/llama-3.3-70b-instruct:free","model_name":"Meta: Llama 3.3 70B Instruct (free)","openrouter_id":"meta-llama/llama-3.3-70b-instruct:free","price_per_million_input":0,"price_per_million_output":0,"provider":"meta-llama","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.2,"swe_bench_verified":32.5},{"bigcodebench_full":42.5,"bigcodebench_hard":28.4,"composite_score":54.31,"context_length":131072,"description":"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n[Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)","huggingface_id":"meta-llama/Llama-3.3-70B-Instruct","humaneval_plus":88.2,"is_free":false,"livebench_coding":48.2,"livecodebench":28.5,"mbpp_plus":74.8,"model_id":"meta-llama/llama-3.3-70b-instruct","model_name":"Meta: Llama 3.3 70B Instruct","openrouter_id":"meta-llama/llama-3.3-70b-instruct","price_per_million_input":0.09999999999999999,"price_per_million_output":0.32,"provider":"meta-llama","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.2,"swe_bench_verified":32.5},{"bigcodebench_full":56.2,"bigcodebench_hard":42.8,"composite_score":53.51,"context_length":128000,"description":"GPT-4o mini Search Preview is a specialized model for web search in Chat Completions. It is trained to understand and execute web search queries.","huggingface_id":"","humaneval_plus":87.2,"is_free":false,"livebench_coding":42.8,"livecodebench":24.8,"mbpp_plus":73.5,"model_id":"openai/gpt-4o-mini-search-preview","model_name":"OpenAI: GPT-4o-mini Search Preview","openrouter_id":"openai/gpt-4o-mini-search-preview","price_per_million_input":0.15,"price_per_million_output":0.6,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":19.2,"swe_bench_verified":22.8},{"bigcodebench_full":56.2,"bigcodebench_hard":42.8,"composite_score":53.51,"context_length":128000,"description":"GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal","huggingface_id":null,"humaneval_plus":87.2,"is_free":false,"livebench_coding":42.8,"livecodebench":24.8,"mbpp_plus":73.5,"model_id":"openai/gpt-4o-mini","model_name":"OpenAI: GPT-4o-mini","openrouter_id":"openai/gpt-4o-mini","price_per_million_input":0.15,"price_per_million_output":0.6,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":19.2,"swe_bench_verified":22.8},{"bigcodebench_full":56.2,"bigcodebench_hard":42.8,"composite_score":53.51,"context_length":128000,"description":"GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal","huggingface_id":null,"humaneval_plus":87.2,"is_free":false,"livebench_coding":42.8,"livecodebench":24.8,"mbpp_plus":73.5,"model_id":"openai/gpt-4o-mini-2024-07-18","model_name":"OpenAI: GPT-4o-mini (2024-07-18)","openrouter_id":"openai/gpt-4o-mini-2024-07-18","price_per_million_input":0.15,"price_per_million_output":0.6,"provider":"openai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":19.2,"swe_bench_verified":22.8},{"bigcodebench_full":40.2,"bigcodebench_hard":25.4,"composite_score":53.23,"context_length":32768,"description":"Qwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).","huggingface_id":"Qwen/Qwen2.5-72B-Instruct","humaneval_plus":86.4,"is_free":false,"livebench_coding":48.5,"livecodebench":28.2,"mbpp_plus":73.8,"model_id":"qwen/qwen-2.5-72b-instruct","model_name":"Qwen2.5 72B Instruct","openrouter_id":"qwen/qwen-2.5-72b-instruct","price_per_million_input":0.07,"price_per_million_output":0.26,"provider":"qwen","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":28.4,"swe_bench_verified":32.8},{"bigcodebench_full":52.4,"bigcodebench_hard":38.2,"composite_score":52.89,"context_length":256000,"description":"Mistral's cutting-edge language model for coding released end of July 2025. Codestral specializes in low-latency, high-frequency tasks such as fill-in-the-middle (FIM), code correction and test generation.\n\n[Blog Post](https://mistral.ai/news/codestral-25-08)","huggingface_id":"","humaneval_plus":81.1,"is_free":false,"livebench_coding":48.8,"livecodebench":28.8,"mbpp_plus":70.2,"model_id":"mistralai/codestral-2508","model_name":"Mistral: Codestral 2508","openrouter_id":"mistralai/codestral-2508","price_per_million_input":0.3,"price_per_million_output":0.8999999999999999,"provider":"mistralai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":24.2,"swe_bench_verified":28.5},{"bigcodebench_full":46.8,"bigcodebench_hard":32.5,"composite_score":51.02,"context_length":262144,"description":"Mistral Large 3 2512 is Mistral\u2019s most capable model to date, featuring a sparse mixture-of-experts architecture with 41B active parameters (675B total), and released under the Apache 2.0 license.","huggingface_id":"","humaneval_plus":84.0,"is_free":false,"livebench_coding":44.5,"livecodebench":22.5,"mbpp_plus":71.5,"model_id":"mistralai/mistral-large-2512","model_name":"Mistral: Mistral Large 3 2512","openrouter_id":"mistralai/mistral-large-2512","price_per_million_input":0.5,"price_per_million_output":1.5,"provider":"mistralai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":20.8,"swe_bench_verified":24.5},{"bigcodebench_full":46.8,"bigcodebench_hard":32.5,"composite_score":51.02,"context_length":131072,"description":"Mistral Large 2 2411 is an update of [Mistral Large 2](/mistralai/mistral-large) released together with [Pixtral Large 2411](/mistralai/pixtral-large-2411)\n\nIt provides a significant upgrade on the previous [Mistral Large 24.07](/mistralai/mistral-large-2407), with notable improvements in long context understanding, a new system prompt, and more accurate function calling.","huggingface_id":"","humaneval_plus":84.0,"is_free":false,"livebench_coding":44.5,"livecodebench":22.5,"mbpp_plus":71.5,"model_id":"mistralai/mistral-large-2411","model_name":"Mistral Large 2411","openrouter_id":"mistralai/mistral-large-2411","price_per_million_input":2.0,"price_per_million_output":6.0,"provider":"mistralai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":20.8,"swe_bench_verified":24.5},{"bigcodebench_full":46.8,"bigcodebench_hard":32.5,"composite_score":51.02,"context_length":131072,"description":"This is Mistral AI's flagship model, Mistral Large 2 (version mistral-large-2407). It's a proprietary weights-available model and excels at reasoning, code, JSON, chat, and more. Read the launch announcement [here](https://mistral.ai/news/mistral-large-2407/).\n\nIt supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Its long context window allows precise information recall from large documents.\n","huggingface_id":"","humaneval_plus":84.0,"is_free":false,"livebench_coding":44.5,"livecodebench":22.5,"mbpp_plus":71.5,"model_id":"mistralai/mistral-large-2407","model_name":"Mistral Large 2407","openrouter_id":"mistralai/mistral-large-2407","price_per_million_input":2.0,"price_per_million_output":6.0,"provider":"mistralai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":20.8,"swe_bench_verified":24.5},{"bigcodebench_full":46.8,"bigcodebench_hard":32.5,"composite_score":51.02,"context_length":128000,"description":"This is Mistral AI's flagship model, Mistral Large 2 (version `mistral-large-2407`). It's a proprietary weights-available model and excels at reasoning, code, JSON, chat, and more. Read the launch announcement [here](https://mistral.ai/news/mistral-large-2407/).\n\nIt supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Its long context window allows precise information recall from large documents.","huggingface_id":null,"humaneval_plus":84.0,"is_free":false,"livebench_coding":44.5,"livecodebench":22.5,"mbpp_plus":71.5,"model_id":"mistralai/mistral-large","model_name":"Mistral Large","openrouter_id":"mistralai/mistral-large","price_per_million_input":2.0,"price_per_million_output":6.0,"provider":"mistralai","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":20.8,"swe_bench_verified":24.5},{"bigcodebench_full":38.2,"bigcodebench_hard":24.8,"composite_score":47.52,"context_length":65536,"description":"Hermes 3 is a generalist language model with many improvements over [Hermes 2](/models/nousresearch/nous-hermes-2-mistral-7b-dpo), including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 70B is a competitive, if not superior finetune of the [Llama-3.1 70B foundation model](/models/meta-llama/llama-3.1-70b-instruct), focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.","huggingface_id":"NousResearch/Hermes-3-Llama-3.1-70B","humaneval_plus":80.5,"is_free":false,"livebench_coding":42.8,"livecodebench":20.5,"mbpp_plus":69.4,"model_id":"nousresearch/hermes-3-llama-3.1-70b","model_name":"Nous: Hermes 3 70B Instruct","openrouter_id":"nousresearch/hermes-3-llama-3.1-70b","price_per_million_input":0.3,"price_per_million_output":0.3,"provider":"nousresearch","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":18.8,"swe_bench_verified":22.4},{"bigcodebench_full":38.2,"bigcodebench_hard":24.8,"composite_score":47.52,"context_length":131072,"description":"Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 70B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).","huggingface_id":"meta-llama/Meta-Llama-3.1-70B-Instruct","humaneval_plus":80.5,"is_free":false,"livebench_coding":42.8,"livecodebench":20.5,"mbpp_plus":69.4,"model_id":"meta-llama/llama-3.1-70b-instruct","model_name":"Meta: Llama 3.1 70B Instruct","openrouter_id":"meta-llama/llama-3.1-70b-instruct","price_per_million_input":0.39999999999999997,"price_per_million_output":0.39999999999999997,"provider":"meta-llama","sources":["openrouter","evalplus","swe_bench","bigcodebench","livebench","livecodebench"],"swe_bench_lite":18.8,"swe_bench_verified":22.4},{"bigcodebench_full":null,"bigcodebench_hard":null,"composite_score":45.96,"context_length":131072,"description":"Mistral Medium 3.1 is an updated version of Mistral Medium 3, which is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8\u00d7 lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases.\n\nThe model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3.1 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.","huggingface_id":"","humaneval_plus":76.2,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":66.8,"model_id":"mistralai/mistral-medium-3.1","model_name":"Mistral: Mistral Medium 3.1","openrouter_id":"mistralai/mistral-medium-3.1","price_per_million_input":0.39999999999999997,"price_per_million_output":2.0,"provider":"mistralai","sources":["openrouter","evalplus"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":null,"bigcodebench_hard":null,"composite_score":45.96,"context_length":131072,"description":"Mistral Medium 3 is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8\u00d7 lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases.\n\nThe model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.","huggingface_id":"","humaneval_plus":76.2,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":66.8,"model_id":"mistralai/mistral-medium-3","model_name":"Mistral: Mistral Medium 3","openrouter_id":"mistralai/mistral-medium-3","price_per_million_input":0.39999999999999997,"price_per_million_output":2.0,"provider":"mistralai","sources":["openrouter","evalplus"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":null,"bigcodebench_hard":null,"composite_score":45.92,"context_length":200000,"description":"Claude 3 Haiku is Anthropic's fastest and most compact model for\nnear-instant responsiveness. Quick and accurate targeted performance.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal","huggingface_id":null,"humaneval_plus":75.9,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":67.1,"model_id":"anthropic/claude-3-haiku","model_name":"Anthropic: Claude 3 Haiku","openrouter_id":"anthropic/claude-3-haiku","price_per_million_input":0.25,"price_per_million_output":1.25,"provider":"anthropic","sources":["openrouter","evalplus"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":null,"bigcodebench_hard":null,"composite_score":45.71,"context_length":128000,"description":"command-r-plus-08-2024 is an update of the [Command R+](/models/cohere/command-r-plus) with roughly 50% higher throughput and 25% lower latencies as compared to the previous Command R+ version, while keeping the hardware footprint the same.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).","huggingface_id":null,"humaneval_plus":75.8,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":66.4,"model_id":"cohere/command-r-plus-08-2024","model_name":"Cohere: Command R+ (08-2024)","openrouter_id":"cohere/command-r-plus-08-2024","price_per_million_input":2.5,"price_per_million_output":10.0,"provider":"cohere","sources":["openrouter","evalplus"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":null,"bigcodebench_hard":null,"composite_score":41.14,"context_length":128000,"description":"command-r-08-2024 is an update of the [Command R](/models/cohere/command-r) with improved performance for multilingual retrieval-augmented generation (RAG) and tool use. More broadly, it is better at math, code and reasoning and is competitive with the previous version of the larger Command R+ model.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Usage Policy](https://docs.cohere.com/docs/usage-policy) and [SaaS Agreement](https://cohere.com/saas-agreement).","huggingface_id":null,"humaneval_plus":68.2,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":59.8,"model_id":"cohere/command-r-08-2024","model_name":"Cohere: Command R (08-2024)","openrouter_id":"cohere/command-r-08-2024","price_per_million_input":0.15,"price_per_million_output":0.6,"provider":"cohere","sources":["openrouter","evalplus"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":null,"bigcodebench_hard":null,"composite_score":41.12,"context_length":131072,"description":"Mistral-Small-3.2-24B-Instruct-2506 is an updated 24B parameter model from Mistral optimized for instruction following, repetition reduction, and improved function calling. Compared to the 3.1 release, version 3.2 significantly improves accuracy on WildBench and Arena Hard, reduces infinite generations, and delivers gains in tool use and structured output tasks.\n\nIt supports image and text inputs with structured outputs, function/tool calling, and strong performance across coding (HumanEval+, MBPP), STEM (MMLU, MATH, GPQA), and vision benchmarks (ChartQA, DocVQA).","huggingface_id":"mistralai/Mistral-Small-3.2-24B-Instruct-2506","humaneval_plus":68.5,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":59.2,"model_id":"mistralai/mistral-small-3.2-24b-instruct","model_name":"Mistral: Mistral Small 3.2 24B","openrouter_id":"mistralai/mistral-small-3.2-24b-instruct","price_per_million_input":0.06,"price_per_million_output":0.18,"provider":"mistralai","sources":["openrouter","evalplus"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":null,"bigcodebench_hard":null,"composite_score":41.12,"context_length":128000,"description":"Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billion parameters with advanced multimodal capabilities. It provides state-of-the-art performance in text-based reasoning and vision tasks, including image analysis, programming, mathematical reasoning, and multilingual support across dozens of languages. Equipped with an extensive 128k token context window and optimized for efficient local inference, it supports use cases such as conversational agents, function calling, long-document comprehension, and privacy-sensitive deployments. The updated version is [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)","huggingface_id":"mistralai/Mistral-Small-3.1-24B-Instruct-2503","humaneval_plus":68.5,"is_free":true,"livebench_coding":null,"livecodebench":null,"mbpp_plus":59.2,"model_id":"mistralai/mistral-small-3.1-24b-instruct:free","model_name":"Mistral: Mistral Small 3.1 24B (free)","openrouter_id":"mistralai/mistral-small-3.1-24b-instruct:free","price_per_million_input":0,"price_per_million_output":0,"provider":"mistralai","sources":["openrouter","evalplus"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":null,"bigcodebench_hard":null,"composite_score":41.12,"context_length":131072,"description":"Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billion parameters with advanced multimodal capabilities. It provides state-of-the-art performance in text-based reasoning and vision tasks, including image analysis, programming, mathematical reasoning, and multilingual support across dozens of languages. Equipped with an extensive 128k token context window and optimized for efficient local inference, it supports use cases such as conversational agents, function calling, long-document comprehension, and privacy-sensitive deployments. The updated version is [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)","huggingface_id":"mistralai/Mistral-Small-3.1-24B-Instruct-2503","humaneval_plus":68.5,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":59.2,"model_id":"mistralai/mistral-small-3.1-24b-instruct","model_name":"Mistral: Mistral Small 3.1 24B","openrouter_id":"mistralai/mistral-small-3.1-24b-instruct","price_per_million_input":0.03,"price_per_million_output":0.11,"provider":"mistralai","sources":["openrouter","evalplus"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":null,"bigcodebench_hard":null,"composite_score":41.12,"context_length":32768,"description":"Mistral Small 3 is a 24B-parameter language model optimized for low-latency performance across common AI tasks. Released under the Apache 2.0 license, it features both pre-trained and instruction-tuned versions designed for efficient local deployment.\n\nThe model achieves 81% accuracy on the MMLU benchmark and performs competitively with larger models like Llama 3.3 70B and Qwen 32B, while operating at three times the speed on equivalent hardware. [Read the blog post about the model here.](https://mistral.ai/news/mistral-small-3/)","huggingface_id":"mistralai/Mistral-Small-24B-Instruct-2501","humaneval_plus":68.5,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":59.2,"model_id":"mistralai/mistral-small-24b-instruct-2501","model_name":"Mistral: Mistral Small 3","openrouter_id":"mistralai/mistral-small-24b-instruct-2501","price_per_million_input":0.03,"price_per_million_output":0.11,"provider":"mistralai","sources":["openrouter","evalplus"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":26.8,"bigcodebench_hard":14.2,"composite_score":39.25,"context_length":32768,"description":"Qwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).","huggingface_id":"Qwen/Qwen2.5-7B-Instruct","humaneval_plus":72.8,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":62.4,"model_id":"qwen/qwen-2.5-7b-instruct","model_name":"Qwen: Qwen2.5 7B Instruct","openrouter_id":"qwen/qwen-2.5-7b-instruct","price_per_million_input":0.04,"price_per_million_output":0.09999999999999999,"provider":"qwen","sources":["openrouter","evalplus","bigcodebench"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":28.5,"bigcodebench_hard":15.2,"composite_score":39.21,"context_length":32768,"description":"Aion-RP-Llama-3.1-8B ranks the highest in the character evaluation portion of the RPBench-Auto benchmark, a roleplaying-specific variant of Arena-Hard-Auto, where LLMs evaluate each other\u2019s responses. It is a fine-tuned base model rather than an instruct model, designed to produce more natural and varied writing.","huggingface_id":"","humaneval_plus":72.6,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":61.2,"model_id":"aion-labs/aion-rp-llama-3.1-8b","model_name":"AionLabs: Aion-RP 1.0 (8B)","openrouter_id":"aion-labs/aion-rp-llama-3.1-8b","price_per_million_input":0.19999999999999998,"price_per_million_output":0.19999999999999998,"provider":"aion-labs","sources":["openrouter","evalplus","bigcodebench"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":28.5,"bigcodebench_hard":15.2,"composite_score":39.21,"context_length":131072,"description":"Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 8B instruct-tuned version is fast and efficient.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nTo read more about the model release, [click here](https://ai.meta.com/blog/meta-llama-3-1/). Usage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).","huggingface_id":"meta-llama/Meta-Llama-3.1-8B-Instruct","humaneval_plus":72.6,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":61.2,"model_id":"meta-llama/llama-3.1-8b-instruct","model_name":"Meta: Llama 3.1 8B Instruct","openrouter_id":"meta-llama/llama-3.1-8b-instruct","price_per_million_input":0.02,"price_per_million_output":0.03,"provider":"meta-llama","sources":["openrouter","evalplus","bigcodebench"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":34.2,"bigcodebench_hard":20.0,"composite_score":8.94,"context_length":8192,"description":"Gemma 2 27B by Google is an open model built from the same research and technology used to create the [Gemini models](/models?q=gemini).\n\nGemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning.\n\nSee the [launch announcement](https://blog.google/technology/developers/google-gemma-2/) for more details. Usage of Gemma is subject to Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms).","huggingface_id":"google/gemma-2-27b-it","humaneval_plus":null,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":null,"model_id":"google/gemma-2-27b-it","model_name":"Google: Gemma 2 27B","openrouter_id":"google/gemma-2-27b-it","price_per_million_input":0.65,"price_per_million_output":0.65,"provider":"google","sources":["openrouter","bigcodebench"],"swe_bench_lite":null,"swe_bench_verified":null},{"bigcodebench_full":22.5,"bigcodebench_hard":10.1,"composite_score":4.52,"context_length":8192,"description":"Gemma 2 9B by Google is an advanced, open-source language model that sets a new standard for efficiency and performance in its size class.\n\nDesigned for a wide variety of tasks, it empowers developers and researchers to build innovative applications, while maintaining accessibility, safety, and cost-effectiveness.\n\nSee the [launch announcement](https://blog.google/technology/developers/google-gemma-2/) for more details. Usage of Gemma is subject to Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms).","huggingface_id":"google/gemma-2-9b-it","humaneval_plus":null,"is_free":false,"livebench_coding":null,"livecodebench":null,"mbpp_plus":null,"model_id":"google/gemma-2-9b-it","model_name":"Google: Gemma 2 9B","openrouter_id":"google/gemma-2-9b-it","price_per_million_input":0.03,"price_per_million_output":0.09,"provider":"google","sources":["openrouter","bigcodebench"],"swe_bench_lite":null,"swe_bench_verified":null}],"success":true}
